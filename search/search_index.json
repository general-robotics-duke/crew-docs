{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"CREW: Platform for Human-AI Teaming","text":""},{"location":"index.html#introduction","title":"Introduction","text":"<p>CREW is a platform designed to facilitate Human-AI teaming research, engage collaborations from multiple scientific disciplines, with a strong emphasis on human involvement. It includes pre-built tasks for cognitive studies and Human-AI teaming with expandable potentials from our modular design. Following conventional cognitive neuroscience research, CREW also supports multimodal human physiological signal recording for behavior analysis. Moreover, CREW benchmarks real-time human-guided reinforcement learning agents using state-of-the-art algorithms and well-tuned baselines. </p> <p>To get started, check out Getting Started for basic examples and Tutorials for advanced usage.</p>"},{"location":"index.html#overview-of-platform","title":"Overview of Platform","text":"<p>CREW consists of two main subcomponents: <code>Dojo</code> and <code>Algorithms</code>.  These subcomponents work together to create an efficient platform for developers and researchers alike.</p> <p><code>Dojo</code> serves as a Unity package designed specifically to facilitate the development of multiplayer games involving human and AI players. We provide a set of pre-built environments as well as a template for building custom tasks with real-time interaction enabled. </p> <p><code>Algorithms</code>, on the other hand, is a Python package aimed at researchers who wish to create AI agents capable of operating and collaborating with humans within the environments established by <code>Dojo</code>. Offering an intuitive interface, <code>Algorithms</code> ensures maximum flexibility and customizability for the researchers.</p> <p>By working in unison, these two subcomponents create a robust and user-friendly platform for the development of interactive experiences.</p>"},{"location":"index.html#authors","title":"Authors","text":"<p>CREW is a fully open-sourced project developed by General                       Robotics Lab at Duke                       University. </p> Lingyu Zhang Zhengran Ji Boyuan Chen"},{"location":"index.html#funding-acknowledgement","title":"Funding Acknowledgement","text":"<p>This work is supported in part by ARL STRONG program under awards W911NF2320182 and W911NF2220113.</p>"},{"location":"algorithms/intro.html","title":"Algorithms","text":"<p><code>CREW</code> takes every effort to make the algorithms independent of the environments provided, giving as much flexibility as possible to researchers for defining their own algorithms and using whatever libraries they are familiar with. ML-Agents, which centers around this functionaility, wraps up the Unity environments and provides standard interfaces that are common in reinforcement learning research. Additionally, the feature of information channels allows for more diverse feedbacks that researchers may feed to algorithms, leaving space for types beyond just binary rewards.</p> <p>In this part of the documentation, we introduce how to use the interfaces to communicate with the environments from <code>CREW</code>.</p> <p>We then provide human-guided examples implemented by torchrl. In principle, there is no restriction for what libraries to use nor how an algorithm is implemented. It's aimed that the examples get users familiar with the functionalities of <code>CREW</code> and help users to implement their own algorithms with ease.</p>"},{"location":"algorithms/setup.html","title":"Setup Algorithms","text":"<p>Before installing crew algorithms, make sure you have miniconda installed. The following commands should be ran in a conda base environment, using a bash shell. For Windows machines, we recommend using git bash.</p> <p>Under <code>crew-algorithms</code>, run:</p> <pre><code>bash install.sh\n</code></pre> <p>This will create a conda environment named <code>crew</code> and install audio-related pakages. For the rest of the dependencies we will use poetry. To install it, first activate the crew environment with <code>conda activate crew</code>, then:</p> <pre><code>curl -sSL https://install.python-poetry.org | python -\n</code></pre> <p>A message should appear after the installation indicating poetry's bin directory. Typically for Linux and OSX, it is <code>\"$HOME/.local/bin\"</code>; for Windows it is <code>$APPDATA\\\\Python\\\\Scripts</code>. Add this directory to your 'PATH' environment variable and append this command to your shell configuration file. For instance, on Linux:</p> <pre><code>echo \"export PATH=\"$HOME/.local/bin/:$PATH\"\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <p>Check poetry can be properly called with: <pre><code>poetry --version\n</code></pre> Next, simply run:</p> <pre><code>poetry install\n</code></pre> <p>If you want to contribute to this repository, you can install the pre-commit hooks for some nice features: <pre><code>poetry run pre-commit install\n</code></pre></p> <p>If you encounter any issues, check out Troubleshooting.</p>"},{"location":"algorithms/examples/DeepTamer.html","title":"Algorithm Example: Deep TAMER","text":"<p>A natural way for humans to guide an agent's learning is to observe its inputs and provide feedback on its actions. This translates directly to incorporating human feedback into reinforcement learning by assigning human feedback as state-action value. Deep TAMER is a prominent human-guided RL framework that leverages this concept by enabling humans to offer discrete, time-stepped positive or negative feedback. To account for human reaction time, a credit assignment mechanism maps feedback to a window of state-action pairs. A neural network \\(F\\) is trained to estimate the human feedback value, denoted as \\(\\hat{f}_{s,a}\\), for a given state-action pair \\((s, a)\\). The policy then selects the action that maximizes this predicted feedback.</p> <p>The algorithm example we provide here is enhances the original Deep TAMER in numerous ways. First, the original Deep TAMER formulation relies on DQN, which only works with discrete action spaces. We designed a continuous version of Deep TAMER while adopting state-of-the-art reinforcement learning implementation practices. We implement an actor-critic framework to handle continuous action space. Here, the critic is the human feedback estimator \\(F(s, a)\\), directly estimating human feedback instead of a target action value. The actor is parameterized by a neural network \\(A(s) = a\\), aiming to maximize the critic's output. The combined objective is defined as: \\begin{equation}     \\mathcal{L_{\\text{c-deeptamer}}} = ||F(s,A(s)) - f_{s,A(s)}||_2 - F(s, A(s)) \\end{equation} We follow recent advancements in neural architectures and hyperparameter designs. Our strong baseline not only enhances Deep TAMER to continuous actions and recent RL practices but also maintains the core methodology of integrating real-time human feedback into the learning process.</p> <p></p>"},{"location":"dojo/intro.html","title":"Dojo","text":"<p><code>Dojo</code> is an online multiplayer platform designed to facilitate the study of human-AI teams. <code>Dojo</code> is built with Unity, providing an intuitive environment for creating and training AI agents. With <code>Dojo</code>, users can create games where AI agents can collaborate or compete with humans.</p> <p><code>Dojo</code> currently offers four pre-built game examples: Bowling, Find Treasure, 1v1 Hide-and-Seek and N v N Hide-and-Seek. These games have been thoughtfully designed to be user-friendly, easy to interact with, and provide built-in interfaces to connect with AI agents.</p> <p>In addition to the pre-built games, <code>Dojo</code> includes a comprehensive set of tools that make it easy for Unity developers to design and create their own games. Using these tools, developers can quickly and effortlessly set up an online multiplayer game, connect to AI agents, customize gameplay mechanics, add new features and functionalities, and more. </p> <p>Multiple interaction modes and data collection is available with <code>Dojo</code>. Data collected from Unity can be streamed and synchronized through LabStreamingLayer.</p>"},{"location":"dojo/setup.html","title":"Setup Dojo","text":"<p>Start by navigating into <code>crew-dojo</code> <pre><code>cd crew-dojo\n</code></pre></p> <p>Setting up a <code>Dojo</code> environment involves two key components: <code>Unity</code> and <code>Nakama</code>. Unity is a game engine and development platform that provides a range of tools for creating interactive experiences, while Nakama is an open-source server designed for real-time multiplayer games and social apps.</p> <p>The setup process involves configuring the Unity environment and launching Nakama server. Additionally, developers may need to set up and download additional tools, such as Docker, to facilitate the development process.</p>"},{"location":"dojo/setup.html#unity","title":"Unity","text":"<p>Unity is one of the crucial components of this project. Unity is a powerful game engine and development platform used to create a wide range of video games, as well as virtual and augmented reality applications. It offers a variety of features and tools for developers to create interactive experiences, including a visual editor, scripting tools, physics engine, and asset management system. Unity supports multiple platforms, such as Windows, Linux, macOS, WebGL, Android, iOS, and more, making it a popular choice for game development across different devices and platforms.</p> <p>To run the pre-existing games on Windows or Linux, users can download pre-built executables here. This option is suitable for those who are not planning to modify or develop games in <code>Dojo</code>. If you are interested in this option, please feel free to skip the Editor Installation and continue from Nakama.</p>"},{"location":"dojo/setup.html#editor-installation","title":"Editor Installation","text":"<p>Unity is the game engine used to develop all the games in the <code>Dojo</code> repository. To access Unity, users must first register for a free account on the Unity website. Once registered, proceed to the downloads page and select the appropriate installer based on the operating system. For Ubuntu systems, the application can be downloaded from the ubuntu unity website.</p> <p>After installing Unity Hub, users can log in with their registered account. Upon logging in, they will be presented with an interface similar to the following:</p> <p></p> <p>To install a Unity editor through the Unity Hub, begin by selecting the \"Installs\" option located on the left-hand panel of the interface. Then, click on the \"Install Editor\" button located in the top right corner of the screen.</p> <p></p> <p>The version of Unity utilized in the <code>Dojo</code> repository is <code>2021.3.24f1</code>. If this version is not readily available under official releases, users may locate it by selecting the \"Archive\" option. Alternatively, they may visit the Unity download archive to obtain the appropriate version.</p> <p>To proceed with the installation process, download both the game engine and editor.</p> <p>Please note that using a newer version of Unity that begins with <code>2021.3.X</code> should also be compatible with the <code>Dojo</code> repository.</p> <p>While choosing the modules, make sure that Windows (mono), Linux (mono), Mac (Mono) and WebGL builds are selected.</p> <p></p>"},{"location":"dojo/setup.html#open-dojo-project","title":"Open Dojo Project","text":"<p>To add the <code>Dojo</code> repository folder to user's Unity project, first click on the \"Projects\" tab on the Unity Hub side panel. Then, select the \"Add project from disk\" option located in the top right corner and navigate to the <code>Dojo</code> repository folder.</p> <p></p> <p>Upon opening the project in the Unity editor, the necessary packages used in <code>Dojo</code> will be automatically installed on first launch. Feel free to ignore any exceptions about <code>.meta</code> files from the console.</p> <p>To access the example games, navigate to the \"Project window\" in the Unity editor and locate the \"Assets/Examples\" folder. The example games can be found within this folder. For instance, to open the Bowling game, double-click on \"Assets/Examples/Bowling/Scenes/Bowling\" in the Project window. Please note that user's editor layouts may differ from the image provided, but the game view window should be similar to the one shown.</p> <p></p>"},{"location":"dojo/setup.html#parrelsync-setup","title":"ParrelSync Setup","text":"<p>In the \"Hierarchy window\", all of the game objects used in the current scene can be found. For example, in Bowling, notice the presence of <code>GameManager_Client</code> and <code>GameManager_Server</code>. These objects are used in multiplayer games over a network, where one game instance runs as a server, authorizing actions from all connected clients. The authoritative server is crucial to ensure that the game view remains consistent across all clients. Therefore, when using Unity Editor, users have the option of playing the game in either server mode or client mode.</p> <p>Due to the need to debug the game by running one editor as server and the other as client, it is highly recommended to setup ParrelSync, a Unity package installed that enables users to launch another Unity editor on the same project.</p> <p>First, locate \"ParrelSync\" on the editor menu. Click \"ParrelSync/Clone Manager\" to open the manager window. Users will have the option to create as many clones of the <code>Dojo</code> project as they like.</p> <p></p> <p>After cloning, click \"Open in New Editor\" to launch another editor window. Then, in \"Hierarchy window\", disable GameManager_Server and enable GameManager_Client to play in client mode. Notice that only one of the two managers will be need to be enabled at a time.</p>"},{"location":"dojo/setup.html#build-a-game","title":"Build a Game","text":"<p>To generate a game build, locate \"DojoBuilder\" in editor menu and open \"Dojo Example Builder\" window.</p> <p></p> <p>Enable the games to build and customize the builder configurations. By default, a \"Builds\" folder will be created under <code>Dojo</code> folder and game builds will be exported there.</p>"},{"location":"dojo/setup.html#nakama","title":"Nakama","text":"<p>Nakama is a crucial framework that facilitates game hosting, client identity, and matchmaking. By utilizing Nakama, <code>Dojo</code> can connect clients over the internet and enable them to join the same game match. To begin using Nakama, Docker is necessary.</p>"},{"location":"dojo/setup.html#docker-installation","title":"Docker Installation","text":"<p>Docker is a popular containerization platform that allows developers to package and deploy applications as self-contained units, called containers. Containers provide a lightweight and efficient way to isolate and run applications, ensuring that they can run consistently across different environments and platforms. With Docker, developers can easily build, test, and deploy applications in a reliable and reproducible way, without worrying about conflicts or dependencies.</p> <p>After installing and launching Docker service, before setting up Nakama, make sure you have Go with the proper version (1.18) installed. If not, follow these instructions.</p> <p>open \"Nakama\" folder in <code>Dojo</code> repository. Run <code>run.sh</code> or <code>run.bat</code> depending on the operating system. Verify that the container called <code>dojo-nakama-server</code> has started running.</p> <p></p> <p>Note: If nakama container failed, it may be a version change of Nakama packages. To resolve this, remove go.mod and go.sum and run the script again.</p> <p>If an error indicating \"vendor\" folder not exist, this may be caused by go package url. Run the following to resolve it: <pre><code>go env -w GOPROXY=\"https://goproxy.io\"\n</code></pre></p>"},{"location":"dojo/setup.html#nakama-console","title":"Nakama Console","text":"<p>Nakama server comes with a console. After launching the containers, open http://127.0.0.1:7351/ in browser to view the connected users and ongoing matches.</p> <p>The default username is <code>admin</code>. Default password is <code>password</code>.</p>"},{"location":"dojo/setup.html#linux-specific-steps","title":"Linux-specific Steps","text":"<p>On Linux environment, you may need to configure <code>/etc/hosts</code> file and add following line so that <code>host.docker.internal</code> DNS resolute to the Docker container IP address.</p> <pre><code>172.17.0.1    host.docker.internal\n</code></pre> <p>By default, each server built Unity instance will try to connect Docker via <code>host.docker.internal</code> address. Make sure this path exists for the system environment where the Instance Server will be running.</p>"},{"location":"dojo/components/design.html","title":"Dojo Design","text":"<p>On this page, you will find a comprehensive explanation of the design pattern utilized by <code>Dojo</code>.</p>"},{"location":"dojo/components/design.html#folder-structure","title":"Folder Structure","text":"<p>When working with the <code>Dojo</code> repository, it's important to focus on two key folders:  <code>Nakama</code>, and <code>Unity</code>.</p> <ul> <li><code>Nakama</code> folder is where you'll find the Docker file and Nakama module implementations.</li> <li><code>Unity</code> folder is the core of the <code>Dojo</code> package, as it contains both the package itself and example games. It is also the folder that will be opened by Unity editor.</li> </ul>"},{"location":"dojo/components/design.html#unity-folder","title":"Unity Folder","text":"<p>The code inside the Unity folder is split into two parts: the <code>Dojo</code> package code and the example games code.</p> <ul> <li><code>Unity/Assets/Dojo</code> has all the code implementation for <code>Dojo</code> package.</li> <li><code>Unity/Assets/Examples</code> contains all the example game implementations.</li> </ul>"},{"location":"dojo/components/design.html#dojo-package","title":"Dojo Package","text":"<p><code>Dojo</code> package has following layout: <pre><code>Agent/\nEditor/\nMujoco/\nNakama/\nNetcode/\nPlugins/\nRecording/\nUI/\nDispatcher.cs\nDojoConnection.cs\nDojoMessage.cs\nDojoNetworkRole.cs\n</code></pre></p> <ul> <li><code>Agent</code> folder provide code for Unity ml-agents integration.</li> <li><code>Editor</code> has code that only runs in Unity editor, such as game builder scripts.</li> <li><code>Mujoco</code> contains the integration code for Mujoco physics engine.</li> <li><code>Nakama</code> integrates the Nakama multiplayer framework.</li> <li><code>Netcode</code> integrates the Unity Netcode for GameObjects.</li> <li><code>Plugins</code> contains system-specific binary files that are used in <code>Dojo</code>.</li> <li><code>Recording</code> provides code for recording game play information.</li> <li><code>UI</code> implements the user interface for human players and viewers.</li> </ul> <p>Finally, <code>DojoConnection.cs</code> is the script responsible for orchestrating the package and ensuring smooth operation by connecting all its components.</p>"},{"location":"dojo/components/design.html#dojo-examples","title":"Dojo Examples","text":"<p>The example games follow a specific folder structure pattern:</p> <ul> <li>The parent folder is the name of the game.</li> <li><code>Actions</code> contains all the keyboard/mouse action maps.</li> <li><code>Configs</code> has the configuration files used for this game.</li> <li><code>Materials</code> has the shader/physics materials used in game.</li> <li><code>Prefabs</code> contains the prefabs in Unity.</li> <li><code>Scenes</code> is where the scene files locate.</li> <li><code>Scripts</code> contains all the code in the game.</li> <li><code>Shaders</code> has custom shader implementations.</li> <li><code>Textures</code> contains any image textures used in the game.</li> <li><code>UIBuilder</code> has any custom user interface configuration for this game.</li> </ul> <p>In conclusion, open a game scene by navigating to the <code>Scenes</code> folder.</p>"},{"location":"dojo/components/design.html#dojo-components","title":"Dojo Components","text":"<p>A standard <code>Dojo</code> game is composed of the following elements:</p> <p></p> <p>The game is based on the Nakama framework and incorporates the <code>Dojo</code> Nakama integration. Additionally, it employs a human interface for human interactions, utilizes ml-agents integration for communicating with the <code>Algorithms</code> code, and integrates netcode for game state synchronization.</p>"},{"location":"dojo/components/interface.html","title":"Human Interface","text":"<p><code>Dojo</code> offers a pre-built user interface, known as <code>DojoMenu</code>, that enables human players or viewers to interact with the program in an intuitive manner.</p>"},{"location":"dojo/components/interface.html#nakama-login","title":"Nakama Login","text":"<p>When launching a client instance, a login panel will be displayed:</p> <p></p> <p>Please note that the \"IP Address\" field refers to the target Nakama server IP address, while the \"Name\" field displays the current client's name on Nakama after login. It's important to keep in mind that the top left and top right panels will still be empty at this point, with the top left panel showing a red bar to indicate that the current client is in a logout state.</p>"},{"location":"dojo/components/interface.html#match-selection","title":"Match Selection","text":"<p>Upon logging in, a list of currently active matches will be presented:</p> <p></p> <p>To request a new match hosted by a server from the Instance Server, the user can click the \"New Match\" button. Alternatively, the user can choose an available match from the list and click \"Join Match\".</p> <p></p>"},{"location":"dojo/components/interface.html#client-information","title":"Client Information","text":"<p>Once the match is joined, the user interface will resemble the following:</p> <p></p> <p>The top left panel shows the display name, the match ID assigned by Nakama, the current client role, and a login status bar. Users can switch between the \"Player\" and \"Viewer\" roles by clicking on the role menu. Clicking on the green status bar will log the user out of Nakama and prompt them to return to the login panel.</p> <p></p> <p>The top right panel displays a list of clients who are actively participating in the same match, along with their display name and current role. By clicking on a client that is AI player, users can select them as feedback targets. Users can also filter the list to only show clients who are currently in a <code>Player</code> role, making it easier to find feedback targets.</p>"},{"location":"dojo/components/mujoco.html","title":"Mujoco Integration","text":"<p>MuJoCo is short for Multi-Joint dynamics with Contact, a physics engine designed for various applications including robotics, biomechanics, graphics and animation, machine learning, and other fields that require precise and quick simulation of articulated structures interacting with their surroundings. Originally created by Roboti LLC, it was later acquired by DeepMind in October 2021 and subsequently made freely accessible. In May 2022, it was also open-sourced, and its codebase is now accessible on the deepmind/mujoco repository on GitHub.</p> <p><code>Dojo</code> provides built-in integration to use Mujoco as its physics engine.</p>"},{"location":"dojo/components/mujoco.html#mujoco-models","title":"Mujoco Models","text":"<p>Any xml model supported by Mujoco can be imported by \"Import MuJoCo Scene\" button.</p> <p></p> <p>A game object will be created in the current scene. Make the object a prefab by dragging it into a project folder.</p>"},{"location":"dojo/components/mujoco.html#prefab-config","title":"Prefab Config","text":"<p>Click on the prefab and attach <code>DojoMujoco</code> script component.</p> <p></p> <p>Click the \"Inject\" button to apply <code>NetworkTransform</code> to all game objects that are Mujoco joints. To ensure smooth object movements, enable interpolation for all <code>NetworkTransform</code> by clicking \"Enable Interpolation\". Lastly, fix any renderer bugs caused by the Mujoco models by clicking \"Fix MjGeom Renderer\".</p>"},{"location":"dojo/components/networking.html","title":"Multiplayer Networking","text":"<p><code>Dojo</code>'s core feature includes networking, allowing any game developed with <code>Dojo</code> to be multiplayer over the Internet by default. This setup enables players and AI agents to access the game from anywhere in the world, greatly enhancing the research's accessibility and encouraging collaborations with individuals beyond the research lab.</p>"},{"location":"dojo/components/networking.html#game-networking","title":"Game Networking","text":"<p>Two packages, Nakama and Unity netcode, enable multiplayer networking in the game. Nakama manages the connection between Unity instances, providing each with a unique identity and allowing them to discover and join the same match for interaction. Meanwhile, Unity netcode builds on top of Nakama as a transport layer, facilitating game state synchronization and ensuring fair gameplay across all clients.</p>"},{"location":"dojo/components/networking.html#networking-model","title":"Networking Model","text":"<p>By default, <code>Dojo</code> utilizes a server-client networking model to facilitate easy game synchronization. This model involves a single server and one or more clients per game. Whenever a client initiates an action, it transmits its command to the server, which then determines the order in which the actions are executed and updates the game state accordingly. Finally, the server broadcasts the latest game state to all clients, ensuring that each client observes the exact same game state.</p> <p>For <code>Dojo</code>, both the server and client connect to the same Nakama server to transport their messages. Although they have different roles in <code>Dojo</code>, they are both clients of the Nakama server.</p> <p></p>"},{"location":"dojo/components/networking.html#match","title":"Match","text":"<p>Nakama offers a robust match-making system that enables users to join independent games and communicate with one another in real-time. This functionality can be likened to a virtual room where players can meet and interact. By leveraging the match system, <code>Dojo</code> is capable of hosting numerous games simultaneously using just a single Nakama server. Players can easily join any game by simply following the corresponding match ID.</p>"},{"location":"dojo/components/networking.html#identity","title":"Identity","text":"<p>Each Unity instance acts as a client to the Nakama server, and upon initial connection, it receives a unique identity from Nakama. This identity is used to track and identify instances within the same game. While Nakama prefers to maintain the persistent identity of each Unity instance, <code>Dojo</code> does not follow this approach. Instead, we have implemented code to automatically delete identity information when an instance disconnects, effectively avoiding this feature.</p>"},{"location":"dojo/components/networking.html#role","title":"Role","text":"<p><code>Dojo</code> facilitates effortless development of human-AI multiplayer games by assigning a unique abstract identity to each Unity instance in the game. The network roles available in the game include:</p> <ul> <li>Server</li> <li>Player</li> <li>Viewer</li> </ul> <p>The <code>Server</code> instance is the primary instance that hosts a Nakama match. This authoritative instance enables other clients to discover and join the same match, allowing them to start playing the game. Only one <code>Server</code> instance is necessary per match.</p> <p>On the other hand, a <code>Player</code> instance is the human client that acts as the <code>player</code> in the game, while a <code>Viewer</code> instance is the human client that can only observe the game without the ability to affect game states. The primary difference between a <code>Player</code> and a <code>Viewer</code> is that a <code>Player</code> has the ability to interact with the game and change game states.</p>"},{"location":"dojo/components/networking.html#configuration","title":"Configuration","text":"<p><code>Dojo</code> offers game-specific configurations in addition to network roles. These configurations allow developers to define the game tag, ensuring that clients only connect to servers running the same game. Additionally, developers can set a maximum number of players for the game, which is useful for developing single-player games like Tetris. While there is no limit to the number of viewers for any game, the maximum number of allowed players is enforced.</p>"},{"location":"dojo/components/networking.html#ai-networking","title":"AI Networking","text":"<p>Nakama has a unique design that facilitates effortless connection to currently operational AI policy programs. Unity ml-agents enables AI networking. Usually, the AI players run on a <code>Server</code> instance, while the <code>Algorithms</code> created AI policy programs operate on the same machine as the <code>Server</code> instance.</p>"},{"location":"dojo/components/networking.html#human-ai-connection","title":"Human-AI Connection","text":"<p>Unity netcode manages the interaction between humans and AI in the game, while the Nakama layer enables human clients to send customized messages to the AI. When a <code>Viewer</code> instance initiates human feedback, the message is sent to the server with the agent ID, which is then transported through Unity ml-agents to the running policy on the <code>Algorithms</code> side.</p>"},{"location":"dojo/components/games/1v1_hide_and_seek.html","title":"1 v 1 Hide and Seek","text":"<p>One-on-one visual hide-and-seek has two players, a hider and a seeker. The seeker's goal is to navigate through the maze and catch the hider. The hider's goal is to remain uncaught throughout the game.</p>"},{"location":"dojo/components/games/1v1_hide_and_seek.html#action-space","title":"Action Space","text":"<p>A 2-dimensional continuous action space:</p> <ul> <li><code>NEXT_DESTINATION_X</code> (0)</li> <li><code>NEXT_DESTINATION_Y</code> (1)</li> </ul>"},{"location":"dojo/components/games/1v1_hide_and_seek.html#observation-space","title":"Observation Space","text":"<p>Hide and Seek provides 3 possible camera views, all 128x128 size with bounding box: <pre><code>Top Left:       (0.0, 0.0)\nWidth Height:   (1.0, 1.0)\n</code></pre></p>"},{"location":"dojo/components/games/1v1_hide_and_seek.html#first-person-view","title":"First Person View","text":"<p>The first person view of the player.</p> <p></p>"},{"location":"dojo/components/games/1v1_hide_and_seek.html#masked-view","title":"Masked View","text":"<p>The top-down view with masked square area around the player.</p> <p></p>"},{"location":"dojo/components/games/1v1_hide_and_seek.html#accumulative-view","title":"Accumulative View","text":"<p>Similar top-down view with masked square area around the player, with the map view of player's visited area.</p> <p></p>"},{"location":"dojo/components/games/1v1_hide_and_seek.html#unity-parameters","title":"Unity Parameters","text":"<p>Game specific parameters:</p> Parameter Name Parameter Format Description <code>-MoveSpeed</code> float Moving speed of the AI agent <code>-RotationSpeed</code> float Rotation speed of the AI agent <code>-EventChannelID</code> string Event side channel assigned by <code>Algorithms</code> <code>-GameStopChannelID</code> string GameStop side channel assigned by <code>Algorithms</code> <code>-DisableFirstCamera</code> none Flag to disable first person camera view <code>-DisableMaskedCamera</code> none Flag to disable masked camera view <code>-DisableAccumuCamera</code> none Flag to disable accumulative camera <code>-MatchStartNumHiders</code> integer Overwrite the number of hiders required to start an episode <code>-MatchStartNumSeekers</code> integer Overwrite the number of seekers required to start an episode <code>-DecisionRequestFrequency</code> float Number of seconds between each two decision requests <p>Universal parameters:</p> Parameter Name Parameter Format Description <code>-NakamaID</code> string Server ID assigned by Instance Server <code>-DojoScreenSize</code> <code>int</code>x<code>int</code> Set window size on launch <code>-DojoScreenPos</code> <code>int</code>,<code>int</code> Set window position on launch <code>-DojoMonitorID</code> integer Set window monitor on launch <code>-CaptureSizeW</code> integer Camera capture size (width) <code>-CaptureSizeH</code> integer Camera capture size (height) <code>-DojoRecording</code> none Flag to enable recording <code>-DojoRecordingFile</code> string To overwrite recording log file path <code>-DojoRecordingIdentity</code> string To overwrite recording log identity <p>User interface controls:</p> Action Name Keyboard Mapping Toggle UI <code>ESC</code>"},{"location":"dojo/components/games/bowling.html","title":"Bowling","text":"<p>Bowling is a classic arcade game released by Atari in 1979. The game simulates the experience of bowling with a trackball controller that allows players to aim and throw the ball down the virtual lane. The objective of the game is to knock down as many pins as possible and score as many points as you can.</p>"},{"location":"dojo/components/games/bowling.html#action-space","title":"Action Space","text":"<p>A 3-dimensional continuous action space:</p> <ul> <li><code>RELEASE POSITION</code> (0)</li> <li><code>DISTANCE TO STEER</code> (1)</li> <li><code>DIRECTION TO STEER</code> (2)</li> </ul>"},{"location":"dojo/components/games/bowling.html#observation-space","title":"Observation Space","text":"<p>320x320 camera view with bounding box: <pre><code>Top Left:       (0.0, 0.5)\nWidth Height:   (1.0, 0.5)\n</code></pre></p>"},{"location":"dojo/components/games/bowling.html#unity-parameters","title":"Unity Parameters","text":"<p>Game specific parameters:</p> Parameter Name Parameter Format Description <code>-NumAgents</code> integer Number of AI agents <code>-EventChannelID</code> string Event side channel assigned by <code>Algorithms</code> <code>-GameStopChannelID</code> string GameStop side channel assigned by <code>Algorithms</code> <p>Universal parameters:</p> Parameter Name Parameter Format Description <code>-NakamaID</code> string Server ID assigned by Instance Server <code>-DojoScreenSize</code> <code>int</code>x<code>int</code> Set window size on launch <code>-DojoScreenPos</code> <code>int</code>,<code>int</code> Set window position on launch <code>-DojoMonitorID</code> integer Set window monitor on launch <code>-CaptureSizeW</code> integer Camera capture size (width) <code>-CaptureSizeH</code> integer Camera capture size (height) <code>-DojoRecording</code> none Flag to enable recording <code>-DojoRecordingFile</code> string To overwrite recording log file path <code>-DojoRecordingIdentity</code> string To overwrite recording log identity"},{"location":"dojo/components/games/bowling.html#human-keyboard-control","title":"Human Keyboard Control","text":"<p>Game play controls:</p> Action Name Keyboard Mapping Move up <code>Up Arrow</code> Move down <code>Down Arrow</code> Shoot <code>Space</code> <p>User interface controls:</p> Action Name Keyboard Mapping Toggle UI <code>ESC</code>"},{"location":"dojo/components/games/find_treasure.html","title":"Find Treasure","text":"<p>The agent is tasked to navigate through a partially observable maze to retrieve a treasure. The agents receive a +10 reward upon reaching the treasure, and a constant -1 time penalty for each step taken. The observation space is a top-down accumulated view of where the agent has explored, initialized by a square area around the agent. The action space is a two-dimensional vector of the next destination coordinates, to which a low-level planning algorithm will navigate the agent. The max episode length is 15 seconds.</p>"},{"location":"dojo/components/games/find_treasure.html#action-space","title":"Action Space","text":"<p>A 2-dimensional continuous action space:</p> <ul> <li><code>NEXT_DESTINATION_X</code> (0)</li> <li><code>NEXT_DESTINATION_Y</code> (1)</li> </ul>"},{"location":"dojo/components/games/find_treasure.html#observation-space","title":"Observation Space","text":"<p>320x320 camera view with bounding box: <pre><code>Top Left:       (0.0, 0.5)\nWidth Height:   (1.0, 0.5)\n</code></pre></p>"},{"location":"dojo/components/games/find_treasure.html#unity-parameters","title":"Unity Parameters","text":"<p>Game specific parameters:</p> Parameter Name Parameter Format Description <code>-NumAgents</code> integer Number of AI agents <code>-EventChannelID</code> string Event side channel assigned by <code>Algorithms</code> <code>-GameStopChannelID</code> string GameStop side channel assigned by <code>Algorithms</code> <p>Universal parameters:</p> Parameter Name Parameter Format Description <code>-NakamaID</code> string Server ID assigned by Instance Server <code>-DojoScreenSize</code> <code>int</code>x<code>int</code> Set window size on launch <code>-DojoScreenPos</code> <code>int</code>,<code>int</code> Set window position on launch <code>-DojoMonitorID</code> integer Set window monitor on launch <code>-CaptureSizeW</code> integer Camera capture size (width) <code>-CaptureSizeH</code> integer Camera capture size (height) <code>-DojoRecording</code> none Flag to enable recording <code>-DojoRecordingFile</code> string To overwrite recording log file path <code>-DojoRecordingIdentity</code> string To overwrite recording log identity <p>User interface controls:</p> Action Name Keyboard Mapping Toggle UI <code>ESC</code>"},{"location":"dojo/components/games/new_game.html","title":"New Game","text":"<p>This section demonstrates some recommended steps to set up a new game environment in <code>Dojo</code>.</p>"},{"location":"dojo/components/games/new_game.html#game-folder","title":"Game Folder","text":"<p>In Unity editor, create the following folder structure for the game.</p> <p></p> <p>Create a scene file under <code>Scenes</code> folder and double click it.</p> <p></p>"},{"location":"dojo/components/games/new_game.html#scene-setup","title":"Scene Setup","text":"<p>Now from the hierarchy panel, create following empty game objects:</p> <p></p> <p>The <code>Environment</code> object will contain all static game objects in the scene. <code>NetworkManager</code> is preserved for Unity's netcode system. <code>DojoClient</code> and <code>DojoServer</code> are two components that serve as the basic layer for establishing a connection with the Nakama server. The <code>GameManager</code> object will hold any scripts specific to the game.</p>"},{"location":"dojo/components/games/new_game.html#nakama","title":"Nakama","text":"<p>For multiplayer networking to function, you'll need to establish a connection with the Nakama server. This can be achieved by configuring the <code>DojoConnection</code> script from the <code>Dojo</code> package for both the server and client components.</p> <p>To get started, select the <code>DojoClient</code> and add the <code>DojoConnection</code> component script via the Inspector panel. Ensure that the \"Is Client\" checkbox is ticked.</p> <p></p> <p>Do the same for <code>DojoServer</code> object, except that \"Is Client\" is not ticked. Now disable <code>DojoClient</code> and this game will run in server mode in the editor.</p> <p></p>"},{"location":"dojo/components/games/new_game.html#netcode","title":"Netcode","text":"<p>Now that the game is able to connect to the Nakama server, it is recommended to setup Unity netcode as well to handle game state synchronizations automatically.</p> <p>Start by creating a child empty game object called <code>Transport</code> within the <code>NetworkManager</code> object. This object will hold the customized <code>DojoTransport</code> that has been built for Unity netcode integration.</p> <p></p> <p>Next, attach the <code>NetworkManager</code> component script from Unity netcode to the <code>NetworkManager</code> object and the <code>DojoTransport</code> script from <code>Dojo</code> to the <code>Transport</code> object.</p> <p></p> <p></p> <p>Then, drag the <code>Transport</code> game object to the \"Network Transport\" entry in the <code>NetworkManager</code> object.</p> <p></p> <p>Lastly, attach the <code>DojoNetcodeHelper</code> script component to the <code>NetworkManager</code> object.</p> <p></p> <p>The above steps enables Unity netcode to funtion by utilizing the Nakama client-client connections as the underlying transport layer.</p>"},{"location":"dojo/components/games/new_game.html#user-interface","title":"User Interface","text":"<p><code>Dojo</code> includes a user-friendly interface that human players can easily interact with.</p> <p>First, locate the <code>DojoMenu</code> prefab from the folder \"Dojo/UI\".</p> <p></p> <p>Then, drag the prefab into the scene and place at the bottom of the hierarchy.</p> <p></p>"},{"location":"dojo/components/games/new_game.html#configurations","title":"Configurations","text":"<p>The last step is to setup configuration files that allow the game to connect with Nakama server and build successfully.</p>"},{"location":"dojo/components/games/new_game.html#nakama-configs","title":"Nakama Configs","text":"<p>Simply right-click within the <code>Configs</code> folder and select \"Create/Dojo/Nakama Match Configurations\" to create a Nakama configuration file.</p> <p></p> <p>To configure your game, simply click on the config file and adjust the \"Game Tag\" and \"Max Num Players\" in the inspector panel to match the specifications of your game.</p> <p>Finally, drag the file into <code>DojoClient</code> and <code>DojoServer</code> script components.</p> <p></p>"},{"location":"dojo/components/games/new_game.html#build-configs","title":"Build Configs","text":"<p><code>Dojo</code> provides easy building scripts to help build the game in both server and client mode for multiple platforms.</p> <p>Same as Nakama config file, right-click in <code>Configs</code> folder and select \"Create/Dojo/BuilderConfig\" to create a build configuration file. Rename the file by the name of your game. Then click the file and fill in the values in inspector panel.</p> <p>\"Scene Path\" is the relative path to the scene file from Unity root folder. In this example, it is the following: <pre><code>Assets/Examples/Template/Scenes/Template.unity\n</code></pre></p> <p>\"Scene Name\" will be the unique game name. \"Output Path\" is the output folder path to store all build artifacts. By default, the output path is <code>../Builds</code>.</p> <p>Finally, the \"Dojo Server Name\" and \"Dojo Client Name\" are the names of the game objects. In this case, <code>DojoServer</code> and <code>DojoClient</code>.</p> <p></p> <p>Finally, add the build config file to the <code>DojoBuilder</code> list from the Unity editor.</p> <p></p>"},{"location":"dojo/components/games/nvn_hide_and_seek.html","title":"N v N Hide and Seek","text":"<p>A multiplayer version of Hide and Seek. The seekers' goal is to navigate through the maze and catch the hiders. The hiders' goal is to remain uncaught throughout the game.</p>"},{"location":"dojo/components/games/nvn_hide_and_seek.html#action-space","title":"Action Space","text":"<p>A 2-dimensional continuous action space:</p> <ul> <li><code>NEXT_DESTINATION_X</code> (0)</li> <li><code>NEXT_DESTINATION_Y</code> (1)</li> </ul>"},{"location":"dojo/components/games/nvn_hide_and_seek.html#observation-space","title":"Observation Space","text":"<p>Hide and Seek provides 3 possible camera views, all 128x128 size with bounding box: <pre><code>Top Left:       (0.0, 0.0)\nWidth Height:   (1.0, 1.0)\n</code></pre></p>"},{"location":"dojo/components/games/nvn_hide_and_seek.html#first-person-view","title":"First Person View","text":"<p>The first person view of the player.</p> <p></p>"},{"location":"dojo/components/games/nvn_hide_and_seek.html#masked-view","title":"Masked View","text":"<p>The top-down view with masked square area around the player.</p> <p></p>"},{"location":"dojo/components/games/nvn_hide_and_seek.html#accumulative-view","title":"Accumulative View","text":"<p>Similar top-down view with masked square area around the player, with the map view of player's visited area.</p> <p></p>"},{"location":"dojo/components/games/nvn_hide_and_seek.html#unity-parameters","title":"Unity Parameters","text":"<p>Game specific parameters:</p> Parameter Name Parameter Format Description <code>-NumHiders</code> integer Number of AI agents as Hiders <code>-NumSeekers</code> integer Number of AI agents as Seekers <code>-MoveSpeed</code> float Moving speed of the AI agent <code>-RotationSpeed</code> float Rotation speed of the AI agent <code>-EventChannelID</code> string Event side channel assigned by <code>Algorithms</code> <code>-GameStopChannelID</code> string GameStop side channel assigned by <code>Algorithms</code> <code>-DisableFirstCamera</code> none Flag to disable first person camera view <code>-DisableMaskedCamera</code> none Flag to disable masked camera view <code>-DisableAccumuCamera</code> none Flag to disable accumulative camera <code>-MatchStartNumHiders</code> integer Overwrite the number of hiders required to start an episode <code>-MatchStartNumSeekers</code> integer Overwrite the number of seekers required to start an episode <code>-DecisionRequestFrequency</code> float Number of seconds between each two decision requests <p>Universal parameters:</p> Parameter Name Parameter Format Description <code>-NakamaID</code> string Server ID assigned by Instance Server <code>-DojoScreenSize</code> <code>int</code>x<code>int</code> Set window size on launch <code>-DojoScreenPos</code> <code>int</code>,<code>int</code> Set window position on launch <code>-DojoMonitorID</code> integer Set window monitor on launch <code>-CaptureSizeW</code> integer Camera capture size (width) <code>-CaptureSizeH</code> integer Camera capture size (height) <code>-DojoRecording</code> none Flag to enable recording <code>-DojoRecordingFile</code> string To overwrite recording log file path <code>-DojoRecordingIdentity</code> string To overwrite recording log identity <p>User interface controls:</p> Action Name Keyboard Mapping Toggle UI <code>ESC</code>"},{"location":"getting_started/human_guided_rl.html","title":"Running Human-guided Reinforcement Learning","text":"<p>A direct way for humans to interact with AI agents to provide feedback on the behavior of the agents. Human-guided Reinforcement Learning focuses on how to enable more effecient agent learning by integrating human guidance. CREW provides interfaces of a variety of feedback types. For instance, Deep TAMER collects discrete binary-valued human feedback and assign it to state-action pairs. To run Deep TAMER on the Bowling environment, navigate under <code>crew-algorithms</code> and activate the <code>crew</code> conda environment. Run the following command:</p> <pre><code>python crew_algorithms/deep_tamer envs=bowling\n</code></pre> <p>The script will launch a server instance of the Bowling game. Then open up the game builds folder <code>crew-dojo/Builds</code> and click on the Bowling server game file under <code>Bowling-Standalone{platform}-Client.</code> This will start a client instance of the game. Log in with an any username and click connect, select the current match, and click <code>Join</code>. You will see a synchronized view of the game in the client instance. On the upper-right corner in <code>Active Clients</code>, click on the AI player. Now you will be able to provide feedback to the agent through the feedback buttons on the right-hand side. Feedback signals collected through this interface will be sent to python and will be used for training. </p> <p>You can also chose to provide feedback through a different machine connected to the same local network. Simply open a client instance on the new machine, enter the IP address of the machine hosting the game in the login menu, and the rest is the same.</p>"},{"location":"getting_started/install.html","title":"Quick Setup","text":"<p>Here we provide the setup instructions for the minimal essenstial components of CREW to help you get started. The following should be ran in a base conda environment in a bash shell. For Windows machines, we recommend using git bash.</p>"},{"location":"getting_started/install.html#1-clone-the-crew-github-repository","title":"1) Clone the CREW github repository:","text":"<pre><code>git clone https://github.com/generalroboticslab/CREW\n</code></pre> <p>It should have the following structure:</p> <pre><code>CREW\n\u251c\u2500\u2500 crew-dojo\n\u251c\u2500\u2500 crew-algorithms\n\u251c\u2500\u2500 Experiment_Pipeline\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"getting_started/install.html#2-create-a-builds-folder-under-crew-dojo","title":"2) Create a <code>Builds</code> folder under <code>crew-dojo</code>.","text":"<p>Download game environments for your OS from drive and move them under <code>crew-dojo/Builds/</code>. Uncompress the zip files so that it has the following structure:</p> <p></p>"},{"location":"getting_started/install.html#3-setup-crew-dojo","title":"3) Setup crew-dojo:","text":""},{"location":"getting_started/install.html#a-download-and-install-docker-desktop","title":"a) Download and install Docker Desktop.","text":"<p>This is essential for networking multiplayer games.</p>"},{"location":"getting_started/install.html#b-install-go-v118","title":"b) Install go v1.18:","text":"<p>Visit the download page go.dev.dl, find version go1.18 under <code>Archived Versions</code>. Download the installation file corresponding to your machine OS and architecture. Install go and make sure it is the correct version : <pre><code>go version\n</code></pre></p>"},{"location":"getting_started/install.html#c-install-and-run-the-docker-image","title":"c) Install and run the docker image:","text":"<p>Under crew-dojo/Nakama, run: <pre><code>bash run.sh\n</code></pre> If successful, it should be running as follows in docker:</p> <p></p>"},{"location":"getting_started/install.html#4-setup-crew-algorithms","title":"4) Setup crew-algorithms:","text":"<p>Under crew-algorithms, run:</p> <pre><code>bash install.sh\n</code></pre> <p>This will create a conda environment named <code>crew</code> and install audio-related pakages. For the rest of the dependencies we will use poetry. To install it, first activate the crew environment with <code>conda activate crew</code>, then:</p> <pre><code>curl -sSL https://install.python-poetry.org | python -\n</code></pre> <p>A message should appear after the installation indicating poetry's bin directory. Typically for Linux and OSX, it is <code>\"$HOME/.local/bin\"</code>; for Windows it is <code>$APPDATA\\\\Python\\\\Scripts</code>. Add this directory to your 'PATH' environment variable and append this command to your shell configuration file. For instance, on Linux:</p> <pre><code>echo \"export PATH=\"$HOME/.local/bin/:$PATH\"\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <p>Check poetry can be properly called with: <pre><code>poetry --version\n</code></pre> Next, simply run:</p> <pre><code>poetry install\n</code></pre> <p>If you encounter any issues, check out Troubleshooting.</p>"},{"location":"getting_started/intro.html","title":"Getting Started","text":"<p>To help get started with CREW, we provide a quick install instruction and a few simple examples including training an RL baseline, human-guided RL, and playing a multiplayer game. </p> <p>These examples are meant to be the minimalistic working examples of working with CREW. For building full-scale customizable human-AI experiments, please refer to Tutorials for more features.</p>"},{"location":"getting_started/multiplayer_game.html","title":"Playing a Multiplayer Game","text":"<p>A core feature of CREW is hosting multiplayer games where players can be humans or AI agents. A prebuilt multiplayer game in CREW is N v N Hide and Seek. </p> <p>To play the game, open the game build folder <code>crew-dojo/Builds</code> and click on the server game file of <code>Hide and Seek</code> under <code>HideAndSeek-Standalone{Platform}-Server</code>. Then for each player, open up a client instance under <code>HideAndSeek-Standalone{Platform}-Client</code> for each computer connected to the same local network as the server. At the login page, type in the IP address of the server computer with any username. Each client instance will be able to see the same server in the match selection menu. Click on it and Join. An empty maze will appear. Go to the menu on the upper-left side, switch from <code>Viewer</code> to <code>Player</code> under the <code>Role</code> dropdown menu. In the waiting room, each client can chose to be a hider or a seeker. Once an equal number of hiders and seekers have been selected, the game will automatically start. Use <code>up</code>, <code>down</code>, <code>left</code>, <code>right</code> keys to control the agent. Once a seeker collides with a hider, the episode ends and will be reset.</p>"},{"location":"getting_started/train_rl_baseline.html","title":"Train an Reinforcement Learning Baseline","text":"<p>RL algorithms are common baselines for measuring AI Agent performance in Human-AI teaming research. Once CREW is installed and the <code>crew</code> conda environment is activated, simply navigate under crew-algorithms and run:</p> <pre><code>python crew_algorithms/ddpg envs=hide_and_seek_1v1 collector.frames_per_batch=240 batch_size=240 train_batches=30\n</code></pre> <p>The script will first open and close a dummy environment to read the environment specs for creating an agent. Then it will launch a server instance of the 1 v 1 Hide and Seek game and train an DDPG algorithm on the seeker agent.</p> <p>By default, CREW uses wandb to handle experiment logging. If you have not used wandb before, you can create an account according to their prompt after running the script. You can also disable wandb logging by adding <code>WANDB_MODE=disabled</code> before the python command:</p> <pre><code>WANDB_MODE=disabled python crew_algorithms/ddpg envs=hide_and_seek_1v1 collector.frames_per_batch=240 batch_size=240 train_batches=30\n</code></pre> <p>The above script will train the agent for 60 minutes. You should be able to observe decent performance by 20-30 minutes.crew</p>"},{"location":"references/release_notes.html","title":"Release Notes","text":""},{"location":"references/release_notes.html#v010","title":"v.0.1.0","text":"<p>Initial Release</p>"},{"location":"references/troubleshooting.html","title":"Troubleshooting","text":"<p>Common issues and mistakes are listed here. If you encounter anything not listed below, please submit an issue on github.</p>"},{"location":"references/troubleshooting.html#installation","title":"Installation","text":""},{"location":"references/troubleshooting.html#installing-algorithms","title":"Installing Algorithms","text":""},{"location":"references/troubleshooting.html#1-pyaudio-is-not-listed-in-conda","title":"1) Pyaudio is not listed in conda:","text":"<p>This could happen if you are using conda for arm (common on OSX), it doesn\u2019t have pyaudio built. You can add packages from the osx-64 subdir to resolve this:</p> <pre><code>conda config --env --set subdir osx-64\n</code></pre>"},{"location":"references/troubleshooting.html#2-pyaudio-installed-failed-missing-portaudio","title":"2) Pyaudio installed failed, missing portaudio.","text":"<p>Some machine will not have portaudio(a c library for recording audio) pre-installed. Running  <code>conda install -c conda-forge pyaudio</code> should automatically resolve this. If not, try installing portaudio manually. For mac: <pre><code>brew install portaudio\n</code></pre></p>"},{"location":"references/troubleshooting.html#3-poetry-not-found","title":"3) Poetry not found","text":"<p>Poetry installed but <code>poetry: command not found</code>. This is likely due to poetry's bin directory not added to your 'PATH' environment variable. After installing poetry, there should be a message in the terminal indicating where poetry's bin directory is located at. Typically it is <code>\"$HOME/.local/bin\"</code> for Linux and OSX, and <code>$APPDATA\\\\Python\\\\Scripts</code> for Windows. Try adding this directory to your 'PATH' environment variable and append this command to your shell configuration file. For instance, on Linux:</p> <pre><code>echo \"export PATH=\"$HOME/.local/bin/:$PATH\"\" &gt;&gt; ~/.bashrc\n</code></pre> <p>and then source it: <pre><code>source ~/.bashrc\n</code></pre></p> <p>Now check if poetry can be properly executed: <pre><code>poetry --version\n</code></pre></p> <p>If this still does not work for you, you can try calling poetry explicitly with <code>\"$HOME/.local/bin/poetry --version</code>.</p>"},{"location":"references/troubleshooting.html#4-python-command-not-found","title":"4) Python: command not found.","text":"<p>Activate the <code>crew</code> enviroment and check the python version:</p> <pre><code>python --version\n</code></pre> <p>If it is not <code>Python 3.10.11</code>, you might have set an alias for python. You can check this by:</p> <pre><code>alias python\n</code></pre> <p>If that is the case, you can either <code>unalias python</code> or explicitly call <code>python3.10</code> for experiments in CREW.</p>"},{"location":"references/troubleshooting.html#5-no-module-named-hydra","title":"5) No module named 'hydra'","text":"<p>This indicates that the <code>crew</code> conda environment was not fully installed. Check for any error messages during <code>poetry install</code>.</p>"},{"location":"references/troubleshooting.html#installing-dojo","title":"Installing Dojo","text":""},{"location":"references/troubleshooting.html#1-tar-cannot-mkdir-permission-denied","title":"1) Tar: Cannot mkdir permission denied","text":"<p>This can happen when installing go without sudo access. Try adding sudo infront of both commands of the installtion command. For instance, on Linux:</p> <pre><code>sudo rm -rf /usr/local/go &amp;&amp; sudo tar -C /usr/local -xzf go1.18.linux-amd64.tar.gz\n</code></pre>"},{"location":"references/troubleshooting.html#2-error-nakama-go-builder-66-run-go-build-trimpath-modvendor-buildmodeplugin-o-backendso","title":"2) ERROR [nakama go-builder 6/6] RUN go build --trimpath --mod=vendor --buildmode=plugin -o ./backend.so","text":"<p>Make sure you have go version 1.18.</p>"},{"location":"references/troubleshooting.html#3-failed-to-solve-heroiclabsnakama-pluginbuilder3150","title":"3) failed to solve: heroiclabs/nakama-pluginbuilder:3.15.0","text":"<pre><code>failed to solve: heroiclabs/nakama-pluginbuilder:3.15.0: failed to \nauthorize: failed to fetch anonymous token: \nGet \"https://auth.docker.io/token?scope=repository%3Aheroiclabs%\n2Fnakama-pluginbuilder%3Apull&amp;service=registry.docker.io\": write tcp \n[2603:6080:6602:97b8:8895:a40d:9ad:1f85]:51185-&gt;\n[2600:1f18:2148:bc01:a3b0:6734:c617:7c5c]:443: write: socket is not connected\n</code></pre> <p>Check internet connection; disconnect any vpns.</p>"},{"location":"references/troubleshooting.html#runtime","title":"Runtime","text":""},{"location":"references/troubleshooting.html#1-unity-instance-not-launching-after-executing-python-script-or-server-instance-shows-red-bar","title":"1) Unity instance not launching after executing python script, or server instance shows red bar.","text":"<p>Check if the Nakama image is running on docker desktop.</p>"},{"location":"references/troubleshooting.html#2-agent-does-not-recieve-feedback","title":"2) Agent does not recieve feedback.","text":"<p>Make sure you have selected the agent in the <code>Activate Clients</code> panel on the client instance.</p>"},{"location":"references/troubleshooting.html#3-cannot-open-up-multiple-instances-of-the-same-build-on-mac","title":"3) Cannot open up multiple instances of the same build on Mac.","text":"<p>In terminal: <code>open -na Unity</code></p>"},{"location":"references/troubleshooting.html#unity-editor","title":"Unity Editor","text":""},{"location":"references/troubleshooting.html#1-play-mode-pause-after-game-start","title":"1) Play mode pause after game start:","text":"<p>Check if \"Error pause\" is turned on in Console.</p>"},{"location":"references/troubleshooting.html#2-uriformatexception-invalid-uri-the-format-of-the-uri-could-not-be-determined","title":"2) UriFormatException: Invalid URI: The format of the URI could not be determined:","text":"<p>Check if Nakama config is set for Dojo Connection script</p>"},{"location":"references/troubleshooting.html#3-feedback-widgets-do-not-appearing-in-game-play","title":"3) Feedback widgets do not appearing in game play:","text":"<p>Go to HumanInterface Component and check if Elements are all set in the Inspector</p>"},{"location":"references/troubleshooting.html#4-object-not-in-sync-between-server-client","title":"4) Object not in sync between server &amp; client:","text":"<p>Add a NetworkTransform component to the object.</p>"},{"location":"references/troubleshooting.html#5-camera-stops-rendering","title":"5) Camera stops rendering.","text":"<p>This could be a result of assigning the camera to an AIAgent\u2019s sensor. If you would like a camera to render to the AIAgent and the main screen at the same time, create a duplicate of the camera.</p>"},{"location":"references/troubleshooting.html#headless-server","title":"Headless Server","text":""},{"location":"references/troubleshooting.html#1-failed-start-x-server","title":"1) Failed start X server:","text":"<pre><code>(EE) Failed to load module \u201cnvidia\u201d (module does not exist, 0)\n(EE) Failed to load module \u201cast\u201d (module does not exist, 0)\n(EE) parse_vt_settings: Cannot open /dev/tty0 (Permission denied):\n</code></pre> <p>You need to install the right version of nvidia drivers for the server (515, 535 \u2026)</p>"},{"location":"tutorials/adding_terrains.html","title":"Adding Terrains","text":"<p>In CREW, you can add procedural generated terrains to your environments. Here we provide an example of a simple grass terrain.</p> <p>First visit the Unity Asset Store, find <code>Terrain Sample Asset Pack</code> by Unity Technologies.</p> <p></p> <p>Select <code>Add to My Assets</code>. After logging in your Unity account, it will take you to Package Manager in your Unity Editor. Select download and then Import.</p> <p></p> <p>FInd the set of prefabs provided by the package in the Project panel. Copy and paste a prefab into your game folder.</p> <p></p> <p>In <code>Hierarchy</code>, create a terrain object through <code>3D Object - Terrain</code>. Align the terrain on the ground of your environment. Ensure the terrain object is selected, go to Inspector and find <code>Paint Details</code> under the Terrain component. Select <code>Edit Details - Add Detail Mesh</code>. Drag the terrain prefab into <code>Detail Prefab</code>, and click on Apply.</p> <p></p> <p>Then simply drag the mouse to draw a terrain of the prefab.</p> <p></p>"},{"location":"tutorials/build_a_pipeline.html","title":"Building an experiment pipeline","text":"<p>Individual differences among humans can significantly affect their teaming with AI agents. CREW supports a set of cognitive tests to quantify these differences. On the other hand, human-AI teaming experiments involves many steps and stages, including playing instruction videos, cognitive testing and organizing the order of human-AI experiments. In CREW, we provide a pipeline for automating this process. The framework integrates various media files (e.g., instruction videos or pictures), inter-trial intervals, executable Python scripts, and Unity builds, ensuring a smooth and effective workflow. The pipeline requires minimal effort from researchers during proctoring, as a single click initiates the sequential execution of experiments. The pipeline allows restart from interruption points. </p> <p></p> <p>1) To get started, navigate into <code>CREW/Experiment_Pipeline/</code>, add your instruction videos and images to <code>CREW/Experiment_Pipeline/Image_and_video</code>. </p> <p>2) Add the cognitive test game builds to <code>CREW/Experiment_Pipeline/game_build</code>. We provide a set of pre-built tests, summarized below.</p> <p></p> <p>3) Modify <code>CREW/Experiment_Pipeline/Script/Human_subject_test.py</code> to organize the experiment session order and timing. To launch the experiment session, simply run this python script.</p>"},{"location":"tutorials/collect_physiological_data.html","title":"Collecting physiological data","text":"<p>Following conventional cognitive neuroscience research, CREW supports multimodal human physiological signal recording for behavior analysis. We support using Lab Streaming Layer (LSL) for collecting and synchronizing data from <code>Unity</code>, <code>Audio</code>, <code>Mouse &amp; Keyboard</code>, <code>Eye Tracking</code>, <code>EEG</code> and <code>ECG</code> devices. Note that although CREW is cross-platform, LSL applications are better supported on Windows, so we provide the relevant applications built for Windows. These apps can be found under <code>CREW/LSL_apps/</code>.</p> <p></p>"},{"location":"tutorials/headless_server.html","title":"Hosting on a headless server","text":"<p>GPU servers can greatly accelerate the training speed of deep learning algorithms. However, most GPU servers are headless and have no default display device. This creates challenges for Unity's visual rendering. We recommend setting up a virtual display tool <code>X server</code> to enable headless rendering.</p> <p>1) Packages Installation: First ensure server has following nvidia packages, assuming NVIDIA driver uses version 515:</p> <pre><code>&gt; apt list --installed | grep \"nvidia\"\n\nlibnvidia-cfg1-515-server/unknown,now 515.65.01-0lambda0~20.04.1 amd64 [installed,automatic]\nlibnvidia-common-515-server/unknown,unknown,now 515.65.01-0lambda0~20.04.1 all [installed,automatic]\nlibnvidia-compute-515-server/unknown,now 515.65.01-0lambda0~20.04.1 amd64 [installed,automatic]\nlibnvidia-compute-515-server/unknown,now 515.65.01-0lambda0~20.04.1 i386 [installed,automatic]\nlibnvidia-decode-515-server/unknown,now 515.65.01-0lambda0~20.04.1 amd64 [installed,automatic]\nlibnvidia-decode-515-server/unknown,now 515.65.01-0lambda0~20.04.1 i386 [installed,automatic]\nlibnvidia-encode-515-server/unknown,now 515.65.01-0lambda0~20.04.1 amd64 [installed,automatic]\nlibnvidia-encode-515-server/unknown,now 515.65.01-0lambda0~20.04.1 i386 [installed,automatic]\nlibnvidia-extra-515-server/unknown,now 515.65.01-0lambda0~20.04.1 amd64 [installed,automatic]\nlibnvidia-extra-515-server/unknown,now 515.65.01-0lambda0~20.04.1 i386 [installed,automatic]\nlibnvidia-fbc1-515-server/unknown,now 515.65.01-0lambda0~20.04.1 amd64 [installed,automatic]\nlibnvidia-fbc1-515-server/unknown,now 515.65.01-0lambda0~20.04.1 i386 [installed,automatic]\nlibnvidia-gl-515-server/unknown,now 515.65.01-0lambda0~20.04.1 amd64 [installed,automatic]\nlibnvidia-gl-515-server/unknown,now 515.65.01-0lambda0~20.04.1 i386 [installed,automatic]\nlibnvidia-ml-dev/unknown,now 11.6.55~11.6.2-0lambda1.1 amd64 [installed,automatic]\nnvidia-compute-utils-515-server/unknown,now 515.65.01-0lambda0~20.04.1 amd64 [installed,automatic]\nnvidia-cuda-dev/unknown,now 11.6.2-0lambda1.1 amd64 [installed,automatic]\nnvidia-cuda-gdb/unknown,now 11.6.124~11.6.2-0lambda1.1 amd64 [installed,automatic]\nnvidia-cuda-toolkit-doc/unknown,unknown,now 11.6.2-0lambda1.1 all [installed,automatic]\nnvidia-cuda-toolkit/unknown,now 11.6.2-0lambda1.1 amd64 [installed]\nnvidia-dkms-515-server/unknown,now 515.65.01-0lambda0~20.04.1 amd64 [installed,automatic]\nnvidia-driver-515-server/unknown,now 515.65.01-0lambda0~20.04.1 amd64 [installed]\nnvidia-kernel-common-515-server/unknown,now 515.65.01-0lambda0~20.04.1 amd64 [installed,automatic]\nnvidia-kernel-source-515-server/unknown,now 515.65.01-0lambda0~20.04.1 amd64 [installed,automatic]\nnvidia-prime/focal-updates,focal-updates,now 0.8.16~0.20.04.2 all [installed]\nnvidia-profiler/unknown,now 11.6.124~11.6.2-0lambda1.1 amd64 [installed,automatic]\nnvidia-settings/unknown,now 515.76-0lambda1 amd64 [installed]\nnvidia-utils-515-server/unknown,now 515.65.01-0lambda0~20.04.1 amd64 [installed,automatic]\n</code></pre> <p>Install following packages: <pre><code>&gt; apt install xorg mesa-utils xserver-xorg xserver-xorg-video-dummy xserver-xorg-video-nvidia-515-server\n</code></pre></p> <p>Then verify that xserver related packages match following:</p> <pre><code>&gt; apt list --installed | grep \"xserver\"\n\nx11-xserver-utils/focal,now 7.7+8 amd64 [installed,automatic]\nxserver-common/focal-updates,focal-updates,focal-security,focal-security,now 2:1.20.13-1ubuntu1~20.04.3 all [installed,automatic]\nxserver-xephyr/focal-updates,focal-security,now 2:1.20.13-1ubuntu1~20.04.3 amd64 [installed,automatic]\nxserver-xorg-core/focal-updates,focal-security,now 2:1.20.13-1ubuntu1~20.04.3 amd64 [installed,automatic]\nxserver-xorg-dev/focal-updates,focal-security,now 2:1.20.13-1ubuntu1~20.04.3 amd64 [installed]\nxserver-xorg-input-all/focal,now 1:7.7+19ubuntu14 amd64 [installed,automatic]\nxserver-xorg-input-libinput/focal,now 0.29.0-1 amd64 [installed,automatic]\nxserver-xorg-input-wacom/focal,now 1:0.39.0-0ubuntu1 amd64 [installed,automatic]\nxserver-xorg-legacy/focal-updates,focal-security,now 2:1.20.13-1ubuntu1~20.04.3 amd64 [installed,automatic]\nxserver-xorg-video-all/focal,now 1:7.7+19ubuntu14 amd64 [installed,automatic]\nxserver-xorg-video-amdgpu/focal-updates,now 19.1.0-1ubuntu0.1 amd64 [installed,automatic]\nxserver-xorg-video-ati/focal,now 1:19.1.0-1 amd64 [installed,automatic]\nxserver-xorg-video-dummy/focal,now 1:0.3.8-1build3 amd64 [installed]\nxserver-xorg-video-fbdev/focal,now 1:0.5.0-1ubuntu1 amd64 [installed,automatic]\nxserver-xorg-video-intel/focal,now 2:2.99.917+git20200226-1 amd64 [installed,automatic]\nxserver-xorg-video-nouveau/focal,now 1:1.0.16-1 amd64 [installed,automatic]\nxserver-xorg-video-nvidia-515-server/unknown,now 515.65.01-0lambda0~20.04.1 amd64 [installed,automatic]\nxserver-xorg-video-qxl/focal,now 0.1.5+git20200331-1 amd64 [installed,automatic]\nxserver-xorg-video-radeon/focal,now 1:19.1.0-1 amd64 [installed,automatic]\nxserver-xorg-video-vesa/focal,now 1:2.4.0-2 amd64 [installed,automatic]\nxserver-xorg-video-vmware/focal,now 1:13.3.0-3 amd64 [installed,automatic]\nxserver-xorg/focal,now 1:7.7+19ubuntu14 amd64 [installed]\n</code></pre> <p>2) Configurations: Modify \u201c/etc/X11/Xwrapper.config\u201d by following line: <pre><code>allowed_users=anybody\n</code></pre></p> <p>This enables all users to start/stop X server. Then prepare two configuration files:</p> <p>headless-dummy.conf</p> <p>headless-gpu.conf</p> <p>Modify \u201cheadless-gpu.conf\u201d by changing BusIDs of the devices. To find BusIDs:</p> <pre><code>nvidia-xconfig --query-gpu-info\n</code></pre> <p>Finally, copy \u201cheadless-gpu.conf\u201d and \u201cheadless-dummy.conf\u201d to \u201c/etc/X11/\u201d</p> <p>3) Start X Server: To start X server on display 0 with GPU:</p> <pre><code>X :0 -config ./headless-gpu.conf\n</code></pre> <p>Test GPU acceleration by: <pre><code>DISPLAY=:0 glxinfo -B\n</code></pre></p> <p>\u201cOpenGL vendor string\u201d should show \u201cNVIDIA Corporation\u201d.</p> <p>To start X server on display 0 with CPU: <pre><code>X :0 -config ./headless-dummy.conf\n</code></pre></p> <p>4) Debugging: Assuming Xserver starts on display 0, and error occurred. Find log info in: <pre><code>&gt; cat /var/log/Xorg.0.log\n</code></pre></p> <p>5) Note that Docker and the Nakama server still needs to be installed on headless servers. In this case Docker Engine should be installed instead of docker desktop. Once docker is installed, you will need to add users to the docker group and reboot the server. Running Nakama is the same as in Setup Nakama. </p> <p>6) Now Unity visuals can be rendered onto X Server. Make sure X is started: <pre><code>&gt; X :0 -config headless-gpu.conf\n</code></pre></p> <p>In case of error: <pre><code>(EE) \nFatal server error:\n(EE) Server is already active for display 0\n    If this server is no longer running, remove /tmp/.X0-lock\n    and start again.\n(EE) \n(EE) \nPlease consult the The X.Org Foundation support \n     at http://wiki.x.org\n for help. \n(EE)\n</code></pre></p> <p>This means X on display 0 has already started. Either use the display or start on a new display index. You can select GPU by openinig <code>bash/etc/X11/headless-gpu.conf</code> , find Section \u201cScreen\u201d, and modify Device name to the target GPU. </p> <p>Now run your python script with:  <pre><code>&gt; DISPLAY=:0 python myscript.py\n</code></pre></p>"},{"location":"tutorials/install.html","title":"Installing CREW","text":"<p>To get started with CREW, clone the github repository: <pre><code>git clone https://github.com/generalroboticslab/CREW\n</code></pre></p> <p>Under <code>CREW</code>, you will find the two main components <code>crew-dojo</code>, <code>crew-algorithms</code> and a folder for setting up human-AI experiments <code>Experiment_Pipeline</code>.</p> <pre><code>CREW\n\u251c\u2500\u2500 crew-dojo\n\u251c\u2500\u2500 crew-algorithms\n\u251c\u2500\u2500 Experiment_Pipeline\n\u2514\u2500\u2500 README.md\n</code></pre> <p>Set each of them up according to the instructions in Dojo Setup and Algorithms Setup.</p>"},{"location":"tutorials/run_an_algorithm.html","title":"Running an Algorithm","text":"<p>All algorithms of CREW are under <code>CREW/crew-algorithms/crew_algorithms/</code>. We provide implementation of state of the art human-guided RL algorithms and baseline RL algorithms. To run launch a training session, first configure the training settings, including environment, number of AI agents and hyperparameters. Then by following the commands in this page, python will automatically start a unity server instance and initialize AI agents.</p>"},{"location":"tutorials/run_an_algorithm.html#configure-training-files","title":"Configure training files","text":"<p>To make things easy, we use Hydra for configurating training sessions. The default configuration for environments are defined in <code>CREW/crew-algorithms/crew_algorithms/envs/configs.py</code>, where you will find game file path, number of agents and other environment specific parameters.</p> <p></p> <p>The default configuration for algorithm specific parameters are under <code>CREW/crew-algorithms/crew_algorithms/conf/</code>.</p> <p></p> <p>Run specific parameters can be configured at the top of each algorithms' <code>__main__.py</code> file, under the <code>Config</code> class.</p> <p></p> <p>To enable wandb logging, modify the <code>entity</code> and <code>project</code> arguments in <code>WandbConfig</code>. To customize the data to be logged, follow their documentation.  You can disable wandb logging by adding <code>WANDB_MODE=disabled</code> before the python command.</p>"},{"location":"tutorials/run_an_algorithm.html#launch-a-training-session","title":"Launch a training session","text":"<p>First enter the <code>crew-algorithms</code> directory:</p> <pre><code>cd crew-algorithms\n</code></pre> <p>Then simply run the command <code>python crew-algorithms/crew_algorithms/{ALGORITHM}</code>, where <code>{ALGORITHM}</code> is the chosen algorithm. For instance:</p> <pre><code>python crew-algorithms/crew_algorithms/ddpg\n</code></pre> <p>You can always overwrite the default configurations by adding arguments in the command line:</p> <pre><code>python crew_algorithms/ddpg envs=hide_and_seek_1v1 collector.frames_per_batch=16 batch_size=16\n</code></pre>"},{"location":"tutorials/run_an_algorithm.html#controlling-training-time","title":"Controlling training time","text":"<p>The decision frequency of all CREW games are set to 2Hz. Parameter <code>collector.frames_per_batch</code> determines how many time steps of data is collected every batch. Parameter <code>train_batches</code> determines how many batches of environment expericence to train on. So the total training time of a session can  be contolled by:</p> <p><code>0.5s * collector.frames_per_batch * train_batches</code></p>"},{"location":"tutorials/select_feedback_modes.html","title":"Select feedback modes","text":"<p>Providing feedback to AI Agents is one of the most direct and common ways of interaction in human-AI teaming. In CREW, we support the following feedback types:</p> <p>1) Discrete scalar feedback</p> <p>A binary positive/ negative feedback given at any time step. </p> <p></p> <p>2) Continuous scalar feedback</p> <p>A continuous valued feedback between [-1, 1], provided at every time step. We provide an interface that allows the user to hover the mouse over a window to provide such feedback</p> <p></p> <p>3) Audio Feedback</p> <p>Direct audio feedback is natural and intuitive for humans. The approach for grounding audio into learning signals is up to the specific algorithm.</p> <p>4) Take Control</p> <p>Directly taking control of an agent and perform teleoperation.</p> <p></p>"},{"location":"tutorials/select_feedback_modes.html#select-a-feedback-interface","title":"Select a feedback interface","text":"<p>We provide pre-built games enabling multiple feedback types. Customized feedback combinations can also be enabled through the Unity Editor. This feature is included in the <code>HumanInterface.cs</code> script. First select <code>HumanInterface</code> in <code>Hierarchy</code>.</p> <p></p> <p>Then in the inspector, find <code>Human Interface (Script)</code>, select the desired feedback types under <code>Elements</code>.</p> <p></p> <p>Build the game and the chosen feedback panels will appear in the client instances:</p> <p></p>"},{"location":"tutorials/start_game_instance.html","title":"Starting a game instance","text":"<p>CREW's environments are built to support mulit-player games. Our networking model involves a single server and one or more clients per game. The server instance hosts the environment, runs the simulation, and handles agent policy training, which can be executed on a powerful headless GPU server. Human participants can connect via client instances on less powerful machines, which display synchronized game states and collect human input. </p> <p>1) To start a game instance, first ensure that Nakama server is running on Docker. If Nakama has not been installed, follow the instructions in Setup Nakama. </p> <p>2) After Nakama is up and running, simply open a server instance of a pre-built unity game under the directory <code>CREW/crew-dojo/Builds/</code>. </p> <p>3) Then open a client instance for each human participant. Log in and select match according to the instructions in Log in a game.</p>"},{"location":"tutorials/start_game_instance.html#adding-ai-agents","title":"Adding AI Agents","text":"<p>AI algorithms are written in python. To enable AI agents in the game, we do not need to manually open a server instance. Instead, the server is automatically openend as part of the python script under <code>Algorithms</code>. Details on how to start a game with AI agents is described in Running an algorithm.</p>"}]}